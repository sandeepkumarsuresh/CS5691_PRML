{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import eigh # for finding the eigenvalue and eigenvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Data/arr_0.npy'\n",
    "data_raw = np.load(data_path)\n",
    "data_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeMean(data):\n",
    "    \"\"\"Input : n dimensional numpy array\n",
    "        Output : mean\"\"\"\n",
    "    return np.sum(data,axis=0)/len(data)\n",
    "\n",
    "def Covariance(data):\n",
    "    \"\"\"Input: numpy array of n-dim\"\"\"\n",
    "    N, M = data.shape\n",
    "    cov = np.zeros((M, M))\n",
    "    for i in range(M):\n",
    "        mean_i = np.sum(data[:, i]) / N\n",
    "        for j in range(M):\n",
    "            mean_j = np.sum(data[:, j]) / N\n",
    "            cov[i, j] = np.sum((data[:, i] - mean_i) * (data[:, j] - mean_j)) / (N - 1)\n",
    "    return cov  \n",
    "\n",
    "def ComputeStd(data):\n",
    "    return np.std(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Mean Centering of the data\"\"\"\n",
    "\n",
    "# Finding the mean of data\n",
    "\n",
    "data_mean = ComputeMean(data_raw)\n",
    "\n",
    "# Mean centering of the data\n",
    "\n",
    "data_centered = data_raw - data_mean\n",
    "\n",
    "std = ComputeStd(data_raw)\n",
    "\n",
    "print(std)\n",
    "\n",
    "data_standard = data_centered/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 8\n",
    "num_cols = 8\n",
    "\n",
    "# Create a subplot with the appropriate number of rows and columns\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(8, 8))\n",
    "\n",
    "# Iterate through the data and plot each image\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        # Reshape the 64-length feature vector into an 8x8 matrix\n",
    "        image = data_centered[i * num_cols + j].reshape(8, 8)\n",
    "\n",
    "        # Display the image in the current subplot\n",
    "        axes[i, j].imshow(image, cmap='gray')\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "# plt.title('Visualization of the data', fontsize=22)\n",
    "plt.savefig('./plots/data_centered')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(data_centered)\n",
    "\n",
    "# Step 3: Calculate the cumulative explained variance ratio\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Step 4: Plot the cumulative explained variance\n",
    "plt.plot(cumulative_variance, marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Variance Explained')\n",
    "plt.title('Cumulative Variance Explained by Principal Components')\n",
    "plt.grid()\n",
    "\n",
    "# Step 5: Determine the number of components that contribute to 90% of the variance\n",
    "n_components_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
    "print(f\"Number of components contributing to 90% of variance: {n_components_90}\")\n",
    "\n",
    "plt.axvline(x=n_components_90, color='r', linestyle='--')\n",
    "plt.axhline(y=0.90, color='r', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Finding the covariance matrix of the centered data\"\"\"\n",
    "\n",
    "data_centered_cov = Covariance(data_centered)\n",
    "data_standard_cov = Covariance(data_standard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Finding the eigenvalue and eigenvector of the covariance matrix\"\"\"\n",
    "eigenvalue , eigenvector =  eigh(data_centered_cov)\n",
    "eigenvalue_std , eigenvector_std =  eigh(data_standard_cov)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_index = np.argsort(eigenvalue)[::-1]\n",
    "sorted_eigenvalue = eigenvalue[sorted_index]\n",
    "sorted_eigenvectors = eigenvector[:,sorted_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_index_std = np.argsort(eigenvalue_std)[::-1]\n",
    "sorted_eigenvalue_std = eigenvalue_std[sorted_index_std]\n",
    "sorted_eigenvectors_std = eigenvector_std[:,sorted_index_std]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_variance = np.sum(sorted_eigenvalue)\n",
    "explained_variance_ratio = sorted_eigenvalue / total_variance\n",
    "print('explained_variance_ratio',explained_variance_ratio)\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Step 6: Plot the cumulative explained variance\n",
    "plt.plot(cumulative_variance, marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Variance Explained')\n",
    "plt.title('Cumulative Variance Explained by Principal Components')\n",
    "plt.grid()\n",
    "\n",
    "# Step 7: Determine the number of components contributing to 90% of the variance\n",
    "n_components_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
    "print(f\"Number of components contributing to 90% of variance: {n_components_90}\")\n",
    "\n",
    "plt.axvline(x=n_components_90, color='r', linestyle='--')\n",
    "plt.axhline(y=0.90, color='r', linestyle='--')\n",
    "plt.savefig('./plots/Cumulative Variance Explained by Principal Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_variance_std = np.sum(sorted_eigenvalue_std)\n",
    "explained_variance_ratio_std = sorted_eigenvalue_std / total_variance_std\n",
    "print('explained_variance_ratio_std',explained_variance_ratio_std)\n",
    "cumulative_variance_std = np.cumsum(explained_variance_ratio_std)\n",
    "\n",
    "# Step 6: Plot the cumulative explained variance\n",
    "plt.plot(cumulative_variance_std, marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Variance Explained')\n",
    "plt.title('Cumulative Variance Explained by Principal Components')\n",
    "plt.grid()\n",
    "\n",
    "# Step 7: Determine the number of components contributing to 90% of the variance\n",
    "n_components_90_std = np.argmax(cumulative_variance_std >= 0.90) + 1\n",
    "print(f\"Number of components contributing to 90% of variance: {n_components_90_std}\")\n",
    "\n",
    "plt.axvline(x=n_components_90_std, color='r', linestyle='--')\n",
    "plt.axhline(y=0.90, color='r', linestyle='--')\n",
    "plt.savefig('./plots/Cumulative Variance Explained by Principal Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 64\n",
    "eig_vals_total = sum(eigenvalue)\n",
    "explained_variance = [(i / eig_vals_total)*100 for i in sorted_eigenvalue]\n",
    "explained_variance = np.round(explained_variance, 2)\n",
    "cum_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print('Explained variance: {}'.format(explained_variance))\n",
    "print('Cumulative explained variance: {}'.format(cum_explained_variance))\n",
    "\n",
    "plt.plot(np.arange(1,n_features+1), cum_explained_variance, '-o')\n",
    "plt.xticks(np.arange(1,n_features+1))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Reconstruction of the data with the 21 eigenvecctors\"\"\"\n",
    "\n",
    "n_PCA_components = 64\n",
    "selected_eigenvectors = sorted_eigenvectors[:, :n_PCA_components]\n",
    "reduced_data = np.dot(data_centered, selected_eigenvectors)\n",
    "reconstructed_data = np.dot(reduced_data, selected_eigenvectors.T)\n",
    "reconstructed_data = (reconstructed_data) + data_mean\n",
    "\n",
    "\n",
    "num_rows = 8\n",
    "num_cols = 8\n",
    "\n",
    "# Create a subplot with the appropriate number of rows and columns\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(8, 8))\n",
    "\n",
    "# Iterate through the data and plot each reconstructed image\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        # Reshape the 64-length feature vector into an 8x8 matrix for both original and reconstructed data\n",
    "        # original_image = data_raw[i * num_cols + j].reshape(8, 8)\n",
    "        reconstructed_image = reconstructed_data[i * num_cols + j].reshape(8, 8)\n",
    "\n",
    "        # Display the original and reconstructed images side by side in the current subplot\n",
    "        # axes[i, j].imshow(original_image, cmap='gray', aspect='auto')\n",
    "        axes[i, j].imshow(reconstructed_image, cmap='viridis', alpha=0.7, aspect='auto')\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "plt.savefig(f'./plots/Reconstruction of the data with the {n_PCA_components} PCA Components')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Reconstruction of the data with the 21 eigenvecctors\"\"\"\n",
    "\n",
    "n_PCA_components = 64\n",
    "selected_eigenvectors_std = sorted_eigenvectors_std[:, :n_PCA_components]\n",
    "reduced_data_std = np.dot(data_standard, selected_eigenvectors_std)\n",
    "reconstructed_data_std = np.dot(reduced_data_std, selected_eigenvectors_std.T)\n",
    "reconstructed_data_std = (reconstructed_data_std * std) + data_mean\n",
    "\n",
    "\n",
    "num_rows = 8\n",
    "num_cols = 8\n",
    "\n",
    "# Create a subplot with the appropriate number of rows and columns\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(8, 8))\n",
    "\n",
    "# Iterate through the data and plot each reconstructed image\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        # Reshape the 64-length feature vector into an 8x8 matrix for both original and reconstructed data\n",
    "        # original_image = data_raw[i * num_cols + j].reshape(8, 8)\n",
    "        reconstructed_image = reconstructed_data_std[i * num_cols + j].reshape(8, 8)\n",
    "\n",
    "        # Display the original and reconstructed images side by side in the current subplot\n",
    "        # axes[i, j].imshow(original_image, cmap='gray', aspect='auto')\n",
    "        axes[i, j].imshow(reconstructed_image, cmap='viridis', alpha=0.7, aspect='auto')\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "plt.savefig(f'./plots/Reconstruction of the data with the {n_PCA_components} PCA Components(standardised)')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [2, 4, 8, 16,21,64]\n",
    "\n",
    "# Dictionary to store the MSE values for each dimension\n",
    "mse_values = {}\n",
    "\n",
    "# Calculate the MSE for each dimension\n",
    "for n_components in dimensions:\n",
    "    # Select the first 'n_components' eigenvectors\n",
    "    selected_eigenvectors = sorted_eigenvectors[:, :n_components]\n",
    "    \n",
    "    # Project the data onto the selected eigenvectors to obtain the reduced representation\n",
    "    reduced_data = np.dot(data_centered, selected_eigenvectors)\n",
    "    \n",
    "    # Reconstruct the data from the reduced representation\n",
    "    reconstructed_data = np.dot(reduced_data, selected_eigenvectors.T)\n",
    "    \n",
    "    # De-standardize the reconstructed data\n",
    "    reconstructed_data = (reconstructed_data) + data_mean\n",
    "    \n",
    "    # Calculate the Mean Square Error (MSE)\n",
    "    mse = np.mean(np.square(data_raw - reconstructed_data))\n",
    "    \n",
    "    # Store the MSE value for this dimension\n",
    "    mse_values[n_components] = mse\n",
    "\n",
    "# Print and interpret the MSE values\n",
    "for n_components, mse in mse_values.items():\n",
    "    print(f\"Dimension {n_components}: MSE = {mse:.4f}\")\n",
    "\n",
    "# Interpret the optimal dimension based on the MSE values\n",
    "optimal_dimension = min(mse_values, key=mse_values.get)\n",
    "print(f\"Optimal dimension based on MSE: {optimal_dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error in terms of eigenvector = avg of remaining eigenvectors\n",
    "\n",
    "# To DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 64\n",
    "W = sorted_eigenvectors[:k, :] # Projection matrix\n",
    "\n",
    "print(W.shape)\n",
    "\n",
    "data_projected = data_centered.dot(W.T)\n",
    "\n",
    "print(data_projected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 8\n",
    "num_cols = 8\n",
    "\n",
    "# Create a subplot with the appropriate number of rows and columns\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(8, 8))\n",
    "\n",
    "# Iterate through the data and plot each image\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        # Reshape the 64-length feature vector into an 8x8 matrix\n",
    "        image = data_projected[i * num_cols + j].reshape(8,8)\n",
    "\n",
    "        # Display the image in the current subplot\n",
    "        axes[i, j].imshow(image, cmap='gray')\n",
    "        axes[i, j].axis('off')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "# plt.title('Visualization of the data', fontsize=22)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "for i in range(20):\n",
    "    ax = fig.add_subplot(5, 5, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(data_raw[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    # label the image with the target value\n",
    "    # ax.text(0, 7, str(y_train[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_egnvalues = sum(sorted_eigenvalue)\n",
    "var_exp = [(i/total_egnvalues) for i in sorted(sorted_eigenvalue, reverse=True)]\n",
    "\n",
    "var_exp = np.array(var_exp)\n",
    "variance_exp_cumsum = var_exp.cumsum().round(2)\n",
    "fig, axes = plt.subplots(1,1,figsize=(16,7), dpi=100)\n",
    "plt.plot(var_exp, color='firebrick')\n",
    "plt.title('Screeplot of Variance Explained %', fontsize=22)\n",
    "plt.xlabel('Number of Principal Components', fontsize=16)\n",
    "# path = '/home/tenet/prml_assignment/variance_max.png'\n",
    "# plt.savefig(path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat = np.cov(a, rowvar = False)\n",
    "cov_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_mat-x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvalue [  0.81663462 181.45636359]\n",
      "eigenvector [[-0.71990351  0.69407416]\n",
      " [ 0.69407416  0.71990351]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-3.783750e+00, -4.335000e+00],\n",
       "       [ 1.152625e+01,  1.434500e+01],\n",
       "       [-1.006375e+01, -1.025500e+01],\n",
       "       [ 1.000625e+01,  9.695000e+00],\n",
       "       [ 4.946250e+00,  3.085000e+00],\n",
       "       [ 4.462500e-01, -5.000000e-03],\n",
       "       [ 2.296250e+00,  2.375000e+00],\n",
       "       [-1.537375e+01, -1.490500e+01]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_Q3 = np.array([\n",
    "    [ 5.51, 5.35 ,3.5],\n",
    "    [ 20.82, 24.03 ,3.5],\n",
    "    [ -0.77, -0.57 ,3.5],\n",
    "    [ 19.30, 19.38 ,3.5],\n",
    "    [ 14.24, 12.77 ,3.5],\n",
    "    [ 9.74, 9.68 ,3.5],\n",
    "    [ 11.59, 12.06 ,3.5],\n",
    "    [ -6.08, -5.22 ,3.5]\n",
    "])\n",
    "\n",
    "data_Q3_mean = ComputeMean(data_Q3)\n",
    "d_=data_Q3-data_Q3_mean\n",
    "data_Q3_cov_ = Covariance(d_)\n",
    "\n",
    "eigenvalue , eigenvector =  eigh(data_Q3_cov_)\n",
    "print('eigenvalue',eigenvalue)\n",
    "\n",
    "print('eigenvector',eigenvector)\n",
    "data_Q3_cov_\n",
    "d_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[20.82],[24.03]])\n",
    "b = np.array([[-.694,-0.72]])\n",
    "x = np.matmul([[20.82],[24.03]],[[-.694,-0.72]])\n",
    "b.shape\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "x = np.array([     5.51,\n",
    "     20.82,\n",
    "     -0.77,\n",
    "     19.30,\n",
    "     14.24,\n",
    "     9.74,\n",
    "     11.59,\n",
    "     -6.08])\n",
    "y = np.array([     5.51,\n",
    "      24.03,\n",
    "      -0.57,\n",
    "      19.38,\n",
    "      12.77,\n",
    "      9.68,\n",
    "      12.06,\n",
    "      -5.22])\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
