\documentclass[solution,addpoints,12pt]{exam}
%\printanswers
\usepackage{amsmath,amssymb,graphicx}
\usepackage{centernot}
\usepackage{hyperref}
\newcommand{\RP}{\ensuremath{\mathsf{RP}}}
\newcommand{\expect}[1]{\ensuremath{\mathbb{E}[#1]}}
\newcommand{\dx}{\mathrm{d}x}
\newcommand{\real}{\mathbb{R}}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

%\documentclass[addpoints,11pt,a4paper]{exam}
\renewcommand{\rmdefault}{ppl} % rm
\linespread{1.05}        % Palatino needs more leading
\usepackage[scaled]{helvet} % ss
\usepackage{courier} % tt
\usepackage{eulervm} % a better implementation of the euler package (not in gwTeX)
\normalfont
\usepackage{caption}
\usepackage[T1]{fontenc}
\usepackage{mathrsfs}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{ulem}
\usepackage{paralist}
\usepackage{amsmath}
\usepackage{psfrag}
\usepackage{fullpage}
\usepackage{fancybox}
\usepackage{ifthen}
\usepackage{hyperref}
\usepackage{float}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage{listings}             % Include the listings-package
\newcommand{\red}[1]{\textcolor{red}{#1}}

\lstset{language=Python}
\usepackage{marvosym}
\usepackage[export]{adjustbox}
\extrawidth{1in}
\usepackage{multicol}
\setlength{\columnsep}{.001cm}
\newcommand{\twopartdef}[4]
{
	\left\{
		\begin{array}{ll}
			#1 & \mbox{if } #2 \\
			#3 & \mbox{if } #4
		\end{array}
	\right.
}
\newcommand{\G}{\mathcal{G}}
\newcommand{\fH}{\mathcal{H}}
\newcommand{\M}{\mathcal{M}}

\begin{document}

\hrule
\vspace{3mm}
\noindent 
{\sf IITM-CS5691 : Pattern Recognition and Machine Learning  \hfill Release Date: Nov 1, 2023}
\\
\noindent 
{\sf Assignment 3 \hfill Due Date: Nov 27, 2023, 23:59\\(~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Note: Assignment is doable by Nov 13 though!)}
%{\sf ~\hfill }
\vspace{3mm}
\hrule
\vspace{3mm}
\noindent{{\sf Roll No:}  \hfill  {\sf Name: SVM Vapnik}}% put your ROLL NO AND NAME HERE

\noindent
{{\sf Collaborators (if any): }} %Names of the collaborators (if any).

\noindent
{{\sf References/sources (if any): 
}} %Reference/source materials, if any.


\vspace{3mm}
\hrule
{\small
\begin{itemize}
\item Use \LaTeX\ to write up your solutions (in the solution blocks of the source \LaTeX\ file of this assignment), submit the resulting rollno.asst2.answers.pdf file at Crowdmark by the due date, and properly drag that pdf's answer pages to the corresponding question in Crowdmark (do this properly, otherwise we won't be able to grade!). (Note: {\bf No late submissions} will be allowed, other than one-day late submission with 10\% penalty or four-day late submission with 30\% penalty.)% Instructions to join Crowdmark and submit your solution to each question within Crowdmark \textbf{TBA} later).
\item Please upload to Moodle a rollno.zip file containing three files:  rollno.asst2.answers.pdf file mentioned above, and two code files for the programming question (rollno.ipynb file and rollno.py file). Do not forget to upload to Crowdmark your results/answers (including Jupyter notebook {\bf with output}) for the programming question.
\item Collaboration is encouraged, but all write-ups must be done individually and independently, and mention your collaborator(s), if any. The same rules apply for codes written for any programming assignments (i.e., write your own code; we will run plagiarism checks on codes).
\item  If you have referred to a book or any other online material or LLMs (Large Language Models like ChatGPT) for obtaining a solution, please cite the source. Again don't copy the source {\it as is} - you may use the source to understand the solution, but write up the solution in your own words (this also means that you cannot copy-paste the solution from LLMs!). Please be advised that {\it the lesser your reliance on online materials or LLMs} for answering the questions, {\it the more your understanding} of the concepts will be and {\it the more prepared you will be for the course exams}.  
\item Points will be awarded based on how clear, concise and rigorous your solutions are, and how correct your answer is. The weightage of this assignment is 12\% towards the overall course grade. 
\end{itemize}
}
\hrule


\begin{questions}
\question[6]{\sc[A direct/discriminant approach to classification]}
For the dataset below, we would like to learn a classifier, specifically a discriminant of the form: $\hat{y}$ = $\text{sign}(wx)$ (assume $\text{sign}(u)=+1$ if $u \ge 0$, and $-1$ otherwise).
\[
\begin{array}{c|c}
x & y \\
\hline
-1 & -1 \\
1 & +1 \\
20 & +1 \\
\end{array}
\]

Let $z_i := z(x_i) := w x_i$. For a training dataset of size $n$, the parameter $w$ of the classifier can be learnt by minimizing the 
\begin{enumerate}
    \item[(L1)] 0-1 loss function aka misclassification error $\sum_{i=1}^{n} (1 - \text{sign}(y_i z_i))/2$,
    
    \item[(L2)] squared loss function $\sum_{i=1}^{n} (y_i - z_i)^2$, or 
    
    \item[(L3)] logistic loss function $\sum_{i=1}^{n} \log(1 + \exp(-y_i z_i))$.
\end{enumerate}

\begin{parts}
    \part[2] The 0-1 loss function is the most intuitive choice to build a good classifier. What value of $w$ will lead to such a good classifier for this dataset: $w=0$ or $w=1$?
%\newpage
\begin{solution}
\end{solution}
%\newpage

\part[4] Between these values of $w$ ($w=0$ vs. $w=1$), determine what value is preferred by the squared and logistic loss functions. Report the actual losses for these $w$ values, and argue which loss function is better.\\
(Note: Optimizing squared loss is equivalent to applying linear regression methodology to solve this classification problem - did it work fine when there are outliers like $x=20$ in the dataset?)
%\newpage
\begin{solution}
\end{solution}
%\newpage
\end{parts}


\question[6]{\sc[Thinking Logistic-ally...]}
Consider the scenario in which a user maintains a dataset consisting of songs that he has downloaded over a period of time. He also tracks the likes (-1)/dislikes(+1) for each song along with a set of features $X_1$, $X_2$. $X_1$ is a binary variable that takes value 1 if the song is sung by his favorite singer, and $X_2$ corresponds to song duration in minutes. This dataset with 10 data points is given below:
\[
[X_1 ~ X_2]=\begin{bmatrix}
1 & 0 & 1 & 0 & 1 & 0 & 0 & 1 & 0 & 0 \\
5 & 10 & 13 & 2 & 3 & 5 & 2 & 10 & 10 & 3 \\
\end{bmatrix}^T
\]
\[
y=\begin{bmatrix}
-1 & -1 & -1 & -1 & -1 & 1 & 1 & 1 & 1 & 1 \\
\end{bmatrix}^T
\]

\begin{parts}
\part[3] Train a logistic regression model on this dataset by performing the gradient descent algorithm steps by setting the initial weights to 0. No bias (intercept term) is required and the step size~ $\eta = 1$. Report the updated weights at the end of two iterations. 
%\newpage
\begin{solution}
\end{solution}
%\newpage

\part[1] What will be the prediction for a new song with features [0, 20] using the trained logistic regression model?
%\newpage
\begin{solution}
\end{solution}
%\newpage
    
\part[2] Discuss if logistic regression is a good choice for addressing this specific problem. If not, what are other better options?
%\newpage
\begin{solution}
\end{solution}
%\newpage
\end{parts}


\question[12] {\sc [SVM's to the rescue]} A Gaussian or Radial Basis Function (RBF) kernel with inverse width $k>0$ is 
\begin{equation*}
    K(u,v)  = e^{-k||u-v||^2}.
\end{equation*}
The below figures show decision boundaries and margins for SVMs learned on the exact same dataset. The parameters used for the different runs are as follows:
\begin{enumerate}
\item[(i)] Linear Kernel with C = 1
\item[(ii)] Linear Kernel with C = 10
\item[(iii)] Linear Kernel with C = 0.1
\item[(iv)] RBF Kernel with  $k=1$, C = 3
\item[(v)] RBF Kernel with $k=0.1$, C = 15
\item[(vi)] RBF Kernel with $k=10$, C = 1
\end{enumerate}
{\bf Find out which figure plot would have resulted after each run mentioned above. Justify your answer.}

In these plots, circles are Class 1, triangles are Class 2, and solid points are support vectors.\\
\includegraphics[scale=0.6]{SVM Images/fig1.PNG}\\
\includegraphics[scale=0.6]{SVM Images/fig2.PNG}
%\newpage
\begin{solution}
\end{solution}
%\newpage


\question[12] {\sc{[Guess-timating Bias and Variance]}}
You are given a dataset consisting of 100 datapoints in \href{https://drive.google.com/drive/folders/1FGEPZnzeK6SvoNVMRIj834ORsT0YMjtk?usp=sharing}{this folder}. You have to fit a polynomial ridge regression model to this data.

As seen in class, a model's error can be decomposed into bias, variance, and noise. A ``learning curve'' provides an opportunity to determine the bias and variance of machine learning models, and to identify models that suffer from high bias (underfitting) or high variance (overfitting). The ``learning curve'' typically shows the training error and validation/testing error on the y-axis and the model complexity on the x-axis. 

\begin{parts}
\part[2] Read the last Section (Section 4) on "Bias and Variance in practice" in this \href{https://cs229.stanford.edu/summer2019/BiasVarianceAnalysis.pdf}{document}, and summarize briefly how you will heuristically find whether your model suffers from (i) high bias, or (ii) high variance, using only the train and validation/test errors of the model. 
%\newpage
\begin{solution}
\end{solution}
%\newpage

\part[2] Start with the code for polynomial regression from the tutorial (the code without in-built package functions in Tutorial \#8) and add quadratic regularization functionality to the code. That is, your code should do polynomial regression with quadratic regularization that takes degree $d$ and regularization parameter $\lambda$ as input. 
\red{Do not use any inbuilt functions from Python packages (except for plotting functions and functions to compute polynomial features for each data point).}
%\newpage
\begin{solution}
\end{solution}
%\newpage

\part[3] Run your code on the provided dataset for degree $d=24$ and each $\lambda$ in the set: 
\[\{10^{-15}, 10^{-9}, 10^{-6}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10^{1}, 10^{2}, 10^{3}, 10^{6}, 10^{9}, 10^{15}\}\]
 \begin{enumerate}
     \item[i.] Perform 5-fold cross-validation on the 100 data points (20 data points in each fold). For each validation fold, compute both training (4-fold-based) and validation (1-fold-based) errors using the mean squared error measure. 
     \item[ii.] Calculate the average training and validation errors across the 5 folds.
 \end{enumerate}
%\newpage
\begin{solution}
\end{solution}
%\newpage
 
 \part[3] Construct a learning curve by plotting the average training and validation errors against the model complexity ($\log_{10} \lambda$). Based on this learning curve, identify the (i) model with the highest bias, (ii) model with the highest variance?, and (iii) the model that will work best on some unseen data.
%\newpage
\begin{solution}
\end{solution}
%\newpage
 
 \part[2] Plot the fitted curve to the given data ($\hat{y}$ against $x$ curve) for the three models reported in part (d) and superimposed with the training and validation data points for any one-fold. 
%\newpage
\begin{solution}
\end{solution}
%\newpage
\end{parts}

Please use the template.ipynb file in the \href{https://drive.google.com/drive/folders/1FGEPZnzeK6SvoNVMRIj834ORsT0YMjtk?usp=sharing}{same folder} to prepare your solution. Provide your results/answers in the pdf file you upload to Crowdmark named rollno.asst3.answers.pdf, and submit your pdf and code separately also in \href{https://coursesnew.iitm.ac.in/mod/assign/view.php?id=27792}{this} moodle link. The pdf+code submitted should be a rollno.zip file containing three files: rollno.asst3.answers.pdf,  rollno.ipynb file (including your code as well as the exact same results/plots uploaded to Crowdmark) and the associated rollno.py file. 
\end{questions}
\end{document}

